<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Python for Statistics and Data Science (part 1)</title>
    <meta charset="utf-8" />
    <meta name="author" content="Guillaume Falmagne" />
    <script src="15_python_stats_files/header-attrs-2.28/header-attrs.js"></script>
    <link href="15_python_stats_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="15_python_stats_files/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="15_python_stats_files/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="styles.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Python for Statistics and Data Science (part 1)
]
.subtitle[
## Simple models and uncertainties
]
.author[
### Guillaume Falmagne
]
.date[
### <br>Nov. 4th, 2024
]

---


# Python's Philosophy

.pull-left[
Python has several core guiding principles:

- **Beautiful:** Code should be aesthetically pleasing and follow the conventions of the community.

- **Explicit:** Clarity is key, avoid being implicit.

- **Simple:** Favor simple, compact and pre-existing solutions.

- **Readability:** Write code that others can easily read and understand.
]
.pull-right[
![:scale 90%](figures/pythonxkcd.png)
]

---
# Code Quality!

### Code Formatters
- **Black**: A *highly* opinionated automatic formatter following a consistent style.
- **autopep8**: Automatically formats code following the PEP 8 style guide (*recommended*). 
- **isort**: Sorts imports alphabetically and automatically separated into sections and by type.

### Linters
They detect code formatting + bad coding practices and styles
- **Flake8**: formatter checking style and quality, good help for clean code.
- **Pylint**: A comprehensive tool to analyze code for errors and enforcing a coding standard (*recommended*).

---

# Python Applications

.left-column60[
- Web Development (Django, Flask)
- Data Analysis (**Pandas**, **NumPy**)
- Machine Learning (PyTorch, TensorFlow, **scikit-learn**)
- Automation and Scripting
- Scientific Computing
- Data Visualization (**Matplotlib**, **Seaborn**, Plotly)
]
.right-column60[
![:scale 90%](figures/scikitlearn.webp)
]
![:scale 61%](figures/matplotlib.png)

---

# Google Colab
Google's cloud-based Jupyter notebook environment: [Google Colab](https://colab.research.google.com/)

- Free access to **GPUs**: Useful for machine learning tasks.
- **No setup** required: Write and execute code directly.
- Easy **sharing**: Just like Google Docs, you can share your Colab notebooks with others.
- Can get much **slower** than on your local machine...

![](figures/ggcolab.png)
- Just click "new notebook" and start coding!
- `Shift + Enter` to run a cell, as in Jupyter notebooks.


---

class: inverse, center, middle
# Python reminders

---

# Reminders: data types
Types are explicit mostly only in numpy

- **int**: Integer numbers
- **float**: Floating-point numbers
- **str**: Strings
- **bool**: Boolean values (True/False) 


``` python
a = np.array([1, 2, 3], dtype=np.int8) # 8 is the precision
b = np.array([1.0, 2.0, 3.0], dtype=np.float32) # 32 is the precision
c = np.array(['a', 'b', 'c'], dtype='str')
d = np.array([True, False, True], dtype=np.bool)
```

---
# Reminders: data structures

- **Lists**: Ordered and mutable (similar to R lists).  
*Note*: to make a numpy array of lists, need to use `dtype=np.object`, so that elements can be of different types and lengths

``` python
a = np.empty(3, dtype=np.object) # but loose most of numpy speed with dtype=object
a[0] = [1, 2, 3]
a[1] = [4, 5]
a[2] = [6, 7, 8, 9]
```

- **Tuples**: Ordered and immutable collection.  
*Note*: tuples are often used to collect arguments or return values

``` python
counts, bins = np.histogram(x) # can also use '_' for unused elements
plt.stairs(counts, bins) # equivalent to plt.hist(h)
```

- **Dictionaries**: Unordered collection of key-value pairs (similar to R lists with named elements).
- **Sets**: Unordered collection of unique (immutable) elements.

``` python
myset = {1, 2, 2, 3, 3} # will contain {1, 2, 3} only
```

---
# Reminders: indexing and control flow

- 0-based indexing + easy slicing:  
`a[0]`, `a[1:3]`,   
`a[a&gt;1]`, `a[~(a&gt;1)]`,  
`a[-1]` (last element),  
`a[::-1]` (reverse order),  
`a[::2]` (every 2nd element) 

- **Control flow**: `if`, `elif`, `else`, `for`, `while`, `break`, `continue`, `pass`

- lambda functions:  
`df.apply(lambda x: x**2) # applies square function to all elements of df`

---
# Reminders: pandas
- Very easy reading:  
`pd.read_csv()`, `pd.read_sql()`, `pd.read_json()`, `pd.read_html()`,  
`pd.read_pickle()` (binary file storing any python object),  
`pd.read_stata()`, ...

Filtering essentials:
- **Columns:** `df['col']` or `df[['col1', 'col2']]`
- **Rows by Position:** `df.iloc[0]` or `df.iloc[:5]`
- **Rows by Label:** `df.loc['index']`
- **Condition:** `df[df['col'] &gt; value]`
- **Combine Rows &amp; Columns:** `df.loc[rows, ['col1', 'col2']]`
- **Scalar Value:** `df.at['row', 'col']` or `df.iat[0, 1]`
- **Query:** `df.query('col1' &gt; 0 &amp; 'col2' &lt; 0)`

Then all the data wrangling: `groupby`, `merge`, `split`, `melt`, `concat`, ...

---
# python cheatsheets

- [numpy cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)
- [pandas cheatsheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)
- [matplotlib many cheatsheets](https://matplotlib.org/cheatsheets/)
- [Seaborne cheatsheet](https://images.datacamp.com/image/upload/v1676302629/Marketing/Blog/Seaborn_Cheat_Sheet.pdf)


---

class: inverse, center, middle
# Statistics essentials

---
# Basic statistics
- nth moment of a distribution: `\(\mu_n = \frac{1}{N} \sum (x_i - c)^n\)`
    - `\(n=1\)`: **mean**, with `\(c=0\)`
    - `\(n=2\)`: **variance**, with `\(c=\bar{x}\)`. Standard deviation ( `\(\sqrt{\text{Var}}\)`) can be approx. by **RMS** (root-mean-square)  
    *Note*: variances (of uncorrelated samples) are **additive**, not standard deviations!
    - `\(n=3\)`: **skewness** = difference between extent of the two 'tails'
    - `\(n=4\)`: **kurtosis** = how 'fat' both tails are compared to a normal distribution

- Quantities related to ranking:
    - **median**: middle value of a sorted list (percentile 50)
    - **mode**: most frequent value ( = peak of histogram)
    - **percentiles**: quantiles that divide the data into 100 parts of equal size

- Covariance: `\(\text{Cov}(X, Y) = \frac{1}{N} \sum (x_i - \bar{x})(y_i - \bar{y})\)`.  
Divide by the product of standard deviations of `\(X\)` and `\(Y\)` to get the correlation coefficient (`df.corr()`).

- numpy functions: `np.mean(x)`, `np.var(x)`, `np.std(x)`, `np.median(x)`,   
`np.percentile(x, [2.5, 25, 50, 75, 97.5])`, ... 

---
# Profiled statistics with `binned_statistic`

.negspace20[
One can transform 2D data, into the statistic of one variable versus the binned other variable
]

.negspace20[


.pull-left[


``` python
df = pd.read_csv("data/sharedprosperity.csv", 
                 skiprows=22) # https://visualizingenergy.org/is-shared-prosperity-connected-to-per-capita-energy-use/
df = df.groupby(df.columns[0]).first()
energy = df['Annualized growth of energy use per capita (%)']
income = df['Annualized growth in mean consumption or income per capita bottom 40']
continent = df['Continent']
sns.scatterplot(x=energy, y=income, 
                hue=continent)
```

&lt;img src="15_python_stats_files/figure-html/unnamed-chunk-8-1.png" width="70%" /&gt;
]
.pull-right[

``` python
from scipy.stats import binned_statistic
_,xedges,yedges,_ = plt.hist2d(energy, income, 
                               bins=13, cmin=1e-2)
bin_means,_,_ = binned_statistic(energy, income, 
                                 statistic='mean', 
                                 bins=xedges)
plt.plot(bin_centers, bin_means, c='red', lw=2)
```
&lt;img src="15_python_stats_files/figure-html/unnamed-chunk-10-3.png" width="75%" /&gt;
]
]

---
# z-score

.negspace20[
![:scale 70%](figures/gauss.webp)
]

.negspace20[
- z-score = **1** standard deviation: **68%** of the data
- z-score = **2** standard deviation: **95% **of the data
- These numbers change when only one side of the distribution is considered  
e.g. `\(3\sigma\)` is **99.9%** (and not **99.7%**) of the data if the null hypothesis is at 0

]

---

# Uncertainties and confidence intervals

.negspace10[
- Conventions for "how wide" the uncertainty bars should be differ a lot between fields!  
**Always say what your uncertainties/confidence intervals are in the caption.**

- Uncertainty ("error") bars are **confidence intervals** that are often set at 95% (2$\sigma$) or 68% (1$\sigma$) confidence level. 
This is the probability that, **if we were to repeat the experiment, the true value would fall within this interval.**

- This is the "**frequentist**" view where the "truth" is the base, and data is distributed around it.  
The "**Bayesian**" considers the data as true, and the model as uncertain.
]

.negspace20[
.left-column60[

``` python
bin_means, _, _ = binned_statistic(energy, 
                                   income, 
                                statistic='mean', bins=13)
bin_stds, _, _ = binned_statistic(energy, 
                                  income, 
                                statistic='std', bins=13)
bin_centers = xedges[1:] - (xedges[1] - xedges[0]) / 2
plt.errorbar(bin_centers, bin_means, 
             yerr=bin_stds, 
             fmt='o', color='red', capsize=5, 
             label='Binned Mean ± 1 SD')
```
]
.right-column60[
&lt;img src="15_python_stats_files/figure-html/unnamed-chunk-12-5.png" width="85%" /&gt;
]
]

---
# Systematic vs statistical uncertainties

- Statistical uncertainties are due to the truly random nature of the data.  
    - **Only these errors are actually Gaussian-distributed and follow the confidence intervals conventions.** 

- **Systematic uncertainties** come from another "truth" element that is **not or wrongly modelled**.  
    - Can come from instruments, human error, or things in the environment that are ignored in the model
    - Often not gaussian-distributed because there is a non-random/systematic process causing them. 
    - But the confidence interval framework is still often used for lack of better understanding of their cause

- **MonteCarlo** (MC) simulation or **sensitivity analysis** can help to estimate systematic uncertainties:
    - E.g. vary an important hyperparameter of your model (e.g. what function you use for regression) and check how your final measurements variables
    - If you know (or have a good guess of) the distribution of this uncertain parameter, you can propagate it:
        - Draw a parameter value from this distribution
        - Run your model with this parameter, get the measurements
        - Repeat this for many ( `\(n&gt;100-1000\)`) parameter values
        - Draw the distribution of resulting measurements and use its standard deviation as uncertainty

---

# Hypothesis testing

- Determines if there is enough evidence to say *"this hypothesis is true for our sampled population"*  
    -- this is what **most papers** try to do!

- **Null Hypothesis** ( `\(H_0\)` ): the default or "no effect" assumption.  
Example: "This drug has no effect on this health indicator."

- **Alternative Hypothesis** ( `\(H_1\)` ): the effect or difference we hope to observe  
Example: "This drug improves this health indicator."

- Process:
    - Formulate hypotheses,  
    - then choose a statistical test and a significance level (e.g. 0.05),  
    - then calculate the test statistic and p-value,
    - finally, reject `\(H_0\)` if p-value &lt; significance level

- Rigourously: **"How likely is it that we would observe this data if the null hypothesis were true?"**

---
# p-value

- **p-value**: The probability of observing a **test statistic as extreme as that of your data**,  
assuming that the null hypothesis is true.
.center[
![:scale 70%](figures/p-value.png)
]

---
# p-value

- **p-value**: The probability of observing a **test statistic as extreme as that of your data**,  
assuming that the null hypothesis is true.

- Beware:
    - p-value is not the probability of the null hypothesis being true
    - **If you test 20 different things, you will probably end up with one p-value&lt;5%  
    even if all null hypotheses are true!**  
    - Important to decide on your hypotheses **before** looking at the data

- In particle physics, the p-value threshold is set to `\(10^{-7}\)` (z-score = 5) and the **global** significance (considering all tests that were made) is often used
- In biology and social sciences, the threshold is often set to 0.05 (z-score = 1.96)

- But what test statistic to use?

---
# Test statistics
- Multiple test statistics can be used:
    - z-test: `\(\frac{\bar{x} - \mu}{\sigma/\sqrt{n}}\)` for comparing the mean of a large population to an expected value
    - When **comparing two scalar values with uncertainties**, very simple formula:  
    **z-score** = `\(\frac{x_1 - x_2}{\sqrt{\sigma_1^2 + \sigma_2^2}}\)`  
    - **Chi-square** test: `\(\chi^2 = \sum \frac{(Obs-Exp)^2}{Exp}\)`, to be compared to a `\(\chi^2\)` distribution with the number of degrees of freedom of the data: `scipy.stats.chi2.pdf(x, n_dof)`  
    That is generally used in the simplest linear regressions.
    - **t-test**: Is there a significant difference between the means of two groups? Works for small groups. E.g. the same population before and after a treatment.  
    `scipy.stats.ttest_ind(a, b, equal_var=False)` (for unequal variances)
    - **F-test/ANOVA**: Are the means of multiple groups different?
    `f_oneway(group1, group2, group3)`
    - **Kolmogorov-Smirnov test**: Are two samples drawn from the same distribution? `scipy.stats.ks_2samp(a, b)`

- Important intuition to check if the formula for your test makes sense:  
the **standard deviation is proportional to `\(\sqrt{n}\)`** (for normal distributions)


---
# Type I and Type II errors in hypothesis testing

.left-column[
.center[
![:scale 65%](figures/type-error.png)

![:scale 72%](figures/type-error-plot.png)
]
]
.right-column[
Choosing an acceptance threshold is always a **trade-off between false positives and false negatives**!

**Power** of a test = `\(1-\beta\)`. Probability of rejecting the null hypothesis when it is actually false.
]

---
# Correlation vs causation

[Spurious correlations by Tyler Vigen](https://www.tylervigen.com/spurious-correlations)

![:scale 63%](figures/spuriouscorr.png)

- Correlations are useful indicators but no proof of causality. 
- In general, need to check against many possible **confounding variables** that might influence the focal variable/measurement (typical multi-linear regression) 
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"chakra": "libs/remark-latest.min.js",
"ratio": "16:10"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(60000);
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
