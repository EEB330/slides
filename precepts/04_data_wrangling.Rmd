---
title: "EEB330 -- Precept 04: Data Wrangling"
author: Scott Wolf, Michelle White
date: Oct. 3, 2024
output: html_document
---

> **Reminder** If you have not installed pak, do so now.  You can install it from CRAN with `install.packages("pak")` Once installed, load it with `library(pak)`.

# Data Wrangling with R

Examples from [Tidy data example](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html).
```{r, message = FALSE}
library(pak)
pak::pkg_install("tidyverse")
library(tidyverse)
```

```{r, message = FALSE}
# Visualizing the structure of the data to understand it better
head(billboard) # billboard is a dataset that comes with the tidyr package
```

## Tidying data

In this step, the `pivot_longer` function is employed to transform the `billboard` dataset from a wide format to a long format. 

- **`names_to = "week"`**: Specifies that the names of the original set of columns (`wk1` to `wk76`) are to be stored in a new column named `week`.
- **`values_to = "rank"`**: Signifies that the values of the original set of columns will be gathered into a new column named `rank`.
- **`values_drop_na = TRUE`**: Ensures that any resulting rows containing `NA` in the `rank` column are omitted from the `billboard2` dataset.

```{r}
billboard2 <- billboard %>%
  pivot_longer(
    wk1:wk76,
    names_to = "week",
    values_to = "rank",
    values_drop_na = TRUE
  )

billboard2
```

Next, the `mutate` function is utilized to create and modify variables within the long-format dataset created in the previous step.

- **`week = as.integer(gsub("wk", "", week))`**: Converts the `week` column to integer by removing the "wk" prefix from the values in the `week` column and then coercing them to integer.
- **`date = as.Date(date.entered) + 7 * (week - 1)`**: Calculates a new `date` column by adding the number of weeks (converted to days) to the `date.entered` column, allowing tracking of the specific date related to each week"s data.
- **`date.entered = NULL`**: Removes the original `date.entered` column after the new `date` column has been created.

```{r}
billboard3 <- billboard2 %>%
  mutate(
    week = as.integer(gsub("wk", "", week)),
    # Adding to dates in R adds days!
    date = as.Date(date.entered) + 7 * (week - 1),
    date.entered = NULL
  )

billboard3
```

Finally, the `arrange` function is applied to organize the dataset based on the `artist`, `track`, and `week` columns. This operation ensures a coherent and ordered display of the dataset, making it more manageable and intuitive for subsequent analysis.
```{r}
long_billboard_sorted <- billboard3 %>% arrange(artist, track, week)

glimpse(long_billboard_sorted)
```

This example code creates a new `song` data frame holding unique `artist` and `track` combinations from the `billboard3` dataframe, and assigns a unique `song_id` to each row (representing each unique song).
```{r, message=FALSE}
song <- billboard3 %>% 
  distinct(artist, track) %>%
  mutate(song_id = row_number())

glimpse(song)
```

The `song` dataframe is then joined with the `billboard3` dataframe to create a new dataframe `rank` that includes the `song_id` column.
```{r, message=FALSE}
rank <- billboard3 %>%
  left_join(song, c("artist", "track")) %>%
  select(song_id, date, week, rank)

glimpse(rank)
```

# Data Wrangling with Python

```{python, message = FALSE}
import pandas as pd
from datetime import timedelta
billboard = pd.read_csv("billboard.csv")
```

```{python, message = FALSE}
# Visualizing the structure of the data to understand it better
billboard.head()
```

## Tidying data

In this step, the `melt` function is employed to transform the `billboard` dataset from a wide format to a long format. 

- **`id_vars`**: Specifies the columns that will remain as identifiers in the long-format dataset.
- **`var_name = "week"`**: Specifies that the names of the original set of columns (`wk1` to `wk76`) are to be stored in a new column named `week`.
- **`value_name = "rank"`**: Signifies that the values of the original set of columns will be gathered into a new column named `rank`.

```{python}
billboard2 = billboard.melt(id_vars = ["artist", "track", "date.entered"], 
                            var_name = "week", 
                            value_name = "rank")

billboard2
```

Next, the `transform` function is utilized to create and modify variables within the long-format dataset created in the previous step.

- **`lambda x: x.str.replace("wk", "").astype(int)`**: Converts the `week` column to integer by removing the "wk" prefix from the values in the `week` column and then coercing them to integer.
- **`pd.to_timedelta(7 * (billboard2["week"] - 1), unit = "D")`**: Calculates a new `date` column by adding the number of weeks (converted to days) to the `date.entered` column, allowing tracking of the specific date related to each week"s data.
- **`drop(columns = ["date.entered"])`**: Removes the original `date.entered` column after the new `date` column has been created.

```{python}
billboard2["week"] = billboard2["week"].transform(lambda x: x.str.replace("wk", "").astype(int))
billboard2["date"] = pd.to_datetime(billboard2["date.entered"]) + pd.to_timedelta(7 * (billboard2["week"] - 1), unit = "D")
billboard3 = billboard2.drop(columns = ["date.entered"])

billboard3
```

Finally, the `sort_values` function is applied to organize the dataset based on the `artist`, `track`, and `week` columns. This operation ensures a coherent and ordered display of the dataset, making it more manageable and intuitive for subsequent analysis.

```{python}
long_billboard_sorted = billboard3.sort_values(by = ["artist", "track", "week"])

long_billboard_sorted
```

This example code creates a new `song` data frame holding unique `artist` and `track` combinations from the `billboard3` dataframe, and assigns a unique `song_id` to each row (representing each unique song).
```{python, message = FALSE}
song = billboard3[["artist", "track"]].drop_duplicates().reset_index(drop = True)
song["song_id"] = song.index + 1

song
```

The `song` dataframe is then merged with the `billboard3` dataframe to create a new dataframe `rank` that includes the `song_id` column.
```{python, message = FALSE}
rank = billboard3.merge(song, on = ["artist", "track"])[["song_id", "date", "week", "rank"]]

rank
```

## Joins/Merges
In this section, we will join the `flights` dataset with the `weather` dataset from the `nycflights13` R package to analyze how weather conditions might have affected the flights.

Make sure you have loaded in `flights.csv` and `weather.csv`!
```{r}
library(nycflights13)  # R has built-in datasets that can be loaded directly from a library
```

```{python}
import pandas as pd
flights = pd.read_csv("flights.csv")
weather = pd.read_csv("weather.csv")
```

#### Inner Join
In an inner join, only the rows with matching keys in both data frames are returned. Rows with non-matching keys are excluded from the result. It's useful when you want to join datasets based on common key columns, and you are only interested in rows with matching keys in both datasets.
```{r}
# R example
flights_weather_inner_joined <- inner_join(flights, weather,by=c("year", "month", "day", "hour", "origin"))
```

```{python}
# Python example
flights_weather_inner_merged = pd.merge(flights, weather, how = "inner", on = ["year", "month", "day", "hour", "origin"])
```

#### Left Join
A left join returns all rows from the left dataset and the matched rows from the right dataset. If there is no match found in the right dataset, then the result will contain `NA`. Use a left join when you want to retain all records from the "left" dataset, and add matching records from the "right" dataset where available.
```{r}
# R example
flights_weather_left_joined <- left_join(flights, weather, by=c("year", "month", "day", "hour", "origin"))
```

```{python}
# Python example
flights_weather_left_merged = pd.merge(flights, weather, how = "left", on = ["year", "month", "day", "hour", "origin"])
```

#### Right Join
In a right join, all rows from the right dataset and the matched rows from the left dataset are returned. If there is no match found in the left dataset, then the result will contain `NA`. It is the opposite of a left join and is used when you want to retain all records from the "right" dataset.
```{r}
# R example
flights_weather_right_joined <- right_join(flights, weather, by=c("year", "month", "day", "hour", "origin"))
```

```{python}
# Python example
flights_weather_right_merged = pd.merge(flights, weather, how = "right", on = ["year", "month", "day", "hour", "origin"])
```

#### Full Join
A full join returns all rows when there is a match in either the left or right dataset. If there is no match found in either dataset, then the result will contain `NA`. It is useful when you want to retain all records from both datasets.
```{r}
# R example
flights_weather_full_joined <- full_join(flights, weather, by=c("year", "month", "day", "hour", "origin"))
head(flights_weather_full_joined)
```

```{python}
# Python example
flights_weather_full_merged = pd.merge(flights, weather, how = "outer", on = ["year", "month", "day", "hour", "origin"])

flights_weather_full_merged.head()
```

#### Using Joins to Analyze the Data
Use the inner joined dataset to calculate the average departure delay for flights with precipitation greater than 0.5.
```{r}
# R example
average_delay_per_condition <- flights_weather_inner_joined %>%
  group_by(precip > 0.5) %>%
  summarise(Average_Departure_Delay = mean(dep_delay, na.rm = TRUE))

average_delay_per_condition
```

```{python}
# Python example
average_delay_per_condition = flights_weather_inner_merged.groupby(flights_weather_inner_merged["precip"] > 0.5).agg(Average_Departure_Delay = ("dep_delay", "mean"))

average_delay_per_condition
```

#### Anti Join
An anti join returns rows from the left dataset where there are no matching keys in the right dataset. It's useful for identifying records in one dataset that do not have a counterpart in another dataset.
```{r}
# R example
flights_weather_anti_joined <- anti_join(flights, weather, by=c("year", "month", "day", "hour", "origin"))

head(flights_weather_anti_joined)
```

In Python, the anti merge operation can be achieved by performing a left merge and then filtering out the rows that have no match in the right dataset. The `indicator` parameter is set to `True` to create the `_merge` column. The `_merge` column is used to identify the rows that have no match in the right dataset with the value `left_only`.
```{python}
# Python example
flights_weather_left_merged = pd.merge(flights, weather, how = "left", on = ["year", "month", "day", "hour", "origin"], indicator = True) 
flights_weather_anti_merged = flights_weather_left_merged[(flights_weather_left_merged["_merge"] == "left_only")].drop("_merge", axis = 1)
flights_weather_anti_merged_cleaned = flights_weather_anti_merged.dropna(axis = 1, how = 'all')

flights_weather_anti_merged_cleaned.head()
```

## Exercises

We will use the weather and flights datasets from the `nycflights13` package (also provided as `.csv` files for Python users) for the exercises.

Please see the [nycflights13"s documentation](https://cran.r-project.org/web/packages/nycflights13/nycflights13.pdf) for more information about the datasets.

### Exercise 1 -- Filtering and Summarizing

- **Task:** Filter the `flights` dataset to include only flights with a delay of more than 12 hours. Group and count this output by `origin` and sort the result in descending order.
- **Expected Output:** A `data.frame` showing the number of flights delayed over 12 hours by airport, ordered from most to least.


### Exercise 2 -- Filtering and Summarizing

- **Task:** Calculate the average air time and the number of flights departing from JFK and arriving at LAX in the `flights` data set. Make sure to report this result in hours.
- **Expected Output:** A `data.frame` with a single row showing the average air time in hours and the number of flights from JFK to LAX.


### Exercise 3 -- Joining/Merging

- **Task:** Perform an inner join between the `airports` and `flights` data sets on the `faa` and `dest` columns, respectively. Then, report the frequency of the time zones of destinations in descending order. Additionally, find an example of an airport with a missing time zone and report the name of the airport, explaining how you checked for it.
- **Expected Output:** 
   1. A `data.frame` listing the time zones by frequency in descending order.
   2. The name of at least one airport with a missing time zone and the code used to identify it.


### Exercise 4: More Wrangling

- **Task:** Identify the top 3 months with the highest average departure delays in the flights dataset. For these months, calculate the average, minimum, and maximum departure delay.
- **Expected Output:** A `data.frame` showing the top 3 months along with their respective average, minimum, and maximum departure delay values.
